{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SCKinect: Motion Tracking for Sound Generation","text":"<p>SCKinect is a SuperCollider plugin that bridges physical movement with sound generation. It allows you to use a Kinect sensor to track human body movements and map them to sound parameters in SuperCollider.</p> <p>CUDA Requirement</p> <p>Note: The current implementation requires CUDA for effective real-time performance. A CPU-only implementation is in progress but not yet fully functional. You will need a CUDA-capable NVIDIA GPU to use this plugin effectively.</p>"},{"location":"#what-is-sckinect","title":"What is SCKinect?","text":"<p>SCKinect integrates multiple technologies to create an interactive system where physical movements can control sound:</p> <ol> <li>Kinect v2 Sensor - A camera device originally designed for Xbox that can track movements and depth.</li> <li>Libfreenect2 - A tool that grabs raw data from the Kinect sensor and makes it available to your computer. It handles the direct communication with the hardware.</li> <li>OpenPose - Computer vision software that can detect human body poses from camera images.</li> <li>SuperCollider - A platform for audio synthesis and algorithmic composition.</li> </ol> <p>The plugin enables you to map specific body joint positions (like hands, knees, or head) to sound parameters, creating an intuitive and embodied way to generate and control sound.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Track up to 25 body joints in real-time</li> <li>Map any joint's X/Y position to sound parameters</li> <li>Choose from different processing pipelines (CUDA/CUDAKDE recommended for performance)</li> <li>Multiple configuration options for tracking quality and performance</li> <li>Simple SuperCollider interface for easy integration with your sound design</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>// Boot the server (needs to be running for the whole UGen to work)\ns.boot;\n// Find and open a Kinect device\nKinect.findAvailable;\nKinect.setPipeline(\"CUDAKDE\");  // CUDA is required for effective performance\nKinect.openDevice(\"YOUR_DEVICE_SERIAL\");\nKinect.start;\n\n// Configure and start body tracking\nKinect.configureTracking(\n  3, 1, \"/path/to/openpose/models\",  // Point to your models folder\n  1, 0, 1, 0.25,\n  0, \"-1x-1\", \"-1x256\",\n  1, \"BODY_25\", 0.5, \n  0.5, 0, 0.05, -1, 0.0\n);\nKinect.startTracking;\n\n// Use right hand's Y position to control sound amplitude\n{\n  SinOsc.ar(\n    440, \n    0, \n    Kinect.kr(0, 1, \"RWrist\", \"Y\")  // Maps Y position to amplitude\n  )\n}.play;\n\n// Later, when done:\nKinect.stopTracking;\nKinect.stop;\nKinect.closeDevice(\"YOUR_DEVICE_SERIAL\");\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>See the Installation and Getting Started guides to begin using SCKinect in your projects. </p>"},{"location":"contributing/","title":"Contributing to SCKinect","text":"<p>Thank you for your interest in contributing to SCKinect! This document provides guidelines and information for contributing to the project.</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<p>There are several ways you can contribute to SCKinect:</p> <ol> <li>Reporting bugs: If you find a bug, please report it by opening an issue on GitHub</li> <li>Suggesting enhancements: Ideas for new features or improvements are welcome</li> <li>Improving documentation: Help clarify and expand the documentation</li> <li>Submitting code: Fix bugs or add new features through pull requests</li> <li>Creating examples: Share your creative uses of SCKinect with others</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<p>To set up a development environment for SCKinect:</p> <ol> <li>Follow the installation guide to install all dependencies</li> <li>Clone the repository with Git:    <pre><code>git clone https://github.com/L42i/SCKinect.git\n</code></pre></li> <li>Create a branch for your changes:    <pre><code>git checkout -b my-feature-branch\n</code></pre></li> </ol>"},{"location":"contributing/#building-for-development","title":"Building for Development","text":"<p>For development, it's helpful to build with debug symbols:</p> <pre><code>mkdir build\ncd build\ncmake .. -DCMAKE_BUILD_TYPE=Debug -DSC_PATH=/path/to/supercollider/source\ncmake --build .\n</code></pre>"},{"location":"contributing/#code-style-guidelines","title":"Code Style Guidelines","text":"<p>When contributing code, please follow these guidelines:</p>"},{"location":"contributing/#c-code","title":"C++ Code","text":"<ul> <li>Use 4 spaces for indentation (no tabs)</li> <li>Follow the existing naming conventions:</li> <li>Class names: <code>CamelCase</code></li> <li>Methods and functions: <code>camelCase</code></li> <li>Variables: <code>camelCase</code> with member variables prefixed with <code>m</code> (e.g., <code>mVariable</code>)</li> <li>Constants and enums: <code>ALL_CAPS</code></li> <li>Add comments for non-obvious code sections</li> <li>Keep lines to a reasonable length (ideally under 100 characters)</li> </ul>"},{"location":"contributing/#supercollider-code","title":"SuperCollider Code","text":"<ul> <li>Use 4 spaces for indentation (no tabs)</li> <li>Follow the existing naming conventions:</li> <li>Class methods: Begin with <code>*</code> (e.g., <code>*methodName</code>)</li> <li>Instance methods: No prefix (e.g., <code>methodName</code>)</li> <li>Variables: <code>camelCase</code></li> <li>Document methods with appropriate argument descriptions</li> </ul>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Make sure your code follows the style guidelines</li> <li>Update documentation as necessary</li> <li>Test your changes thoroughly</li> <li>Submit a pull request describing your changes</li> <li>Wait for feedback from maintainers</li> </ol>"},{"location":"contributing/#testing","title":"Testing","text":"<p>When implementing new features or fixing bugs, please include tests:</p> <ul> <li>For C++ code, add test cases if applicable</li> <li>For SuperCollider code, include example scripts that demonstrate the functionality</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>If your changes affect the user interface or add new features, please update the corresponding documentation. This might include:</p> <ul> <li>Updating method documentation in <code>Kinect.schelp</code></li> <li>Adding examples to demonstrate new features</li> <li>Updating this MkDocs documentation</li> </ul>"},{"location":"contributing/#community-guidelines","title":"Community Guidelines","text":"<p>When participating in the SCKinect community, please:</p> <ul> <li>Be respectful and inclusive of others</li> <li>Focus on the technical merits of ideas</li> <li>Help others learn and grow</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing to SCKinect, you agree that your contributions will be licensed under the same license as the project.</p>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<p>If you need help with contributing:</p> <ul> <li>Open an issue on GitHub with a \"question\" label</li> <li>Reach out to the maintainers directly</li> </ul> <p>Thank you for your contributions to SCKinect! </p>"},{"location":"getting-started/","title":"Getting Started with SCKinect","text":"<p>This guide will walk you through the basic steps of setting up and using SCKinect to connect body movement with sound generation.</p> <p>CUDA Requirement</p> <p>The current implementation requires CUDA for effective real-time performance. The CPU-only implementation is in progress but not yet fully functional. You will need a CUDA-capable NVIDIA GPU to use this plugin as described in this guide.</p>"},{"location":"getting-started/#basic-workflow","title":"Basic Workflow","text":"<p>Working with SCKinect follows this general workflow:</p> <ol> <li>Setup: Find and connect to the Kinect device</li> <li>Configure: Set tracking parameters</li> <li>Track: Start body tracking</li> <li>Map: Map joint positions to sound parameters</li> <li>Generate: Use the mapped values to generate sound</li> <li>Cleanup: Stop tracking and close the device when done</li> </ol>"},{"location":"getting-started/#step-1-setting-up-the-kinect","title":"Step 1: Setting Up the Kinect","text":"<p>First, make sure your Kinect is connected to your computer. Then, in SuperCollider:</p> <pre><code>// Boot the server if it's not already running\ns.boot;\n\n// Find available Kinect devices\nKinect.findAvailable;\n</code></pre> <p>This will print the serial number(s) of any connected Kinect devices to the SuperCollider post window.</p>"},{"location":"getting-started/#step-2-connect-to-the-kinect","title":"Step 2: Connect to the Kinect","text":"<p>Once you have the serial number, you can connect to the device:</p> <pre><code>// Choose a processing pipeline (CUDA is recommended)\nKinect.setPipeline(\"CUDAKDE\");  // \"CUDA\" is also an option but \"CUDAKDE\" generally performs better\n\n// Open the device using its serial number\nKinect.openDevice(\"YOUR_DEVICE_SERIAL\");\n\n// Start the device\nKinect.start;\n</code></pre> <p>Note</p> <p>Replace <code>\"YOUR_DEVICE_SERIAL\"</code> with the actual serial number of your Kinect device.</p> <p>Warning</p> <p>While \"CPU\" is listed as a pipeline option, it is still in development and may not provide adequate performance for real-time tracking. Always use \"CUDAKDE\" or \"CUDA\" for best results.</p>"},{"location":"getting-started/#step-3-configure-body-tracking","title":"Step 3: Configure Body Tracking","text":"<p>Before you can start tracking, you need to configure the tracking parameters:</p> <pre><code>Kinect.configureTracking(\n  // Basic settings\n  loggingLevel: 3,\n  maxPeople: 1,\n  modelFolder: \"/path/to/openpose/models\",\n\n  // GPU settings\n  numGpu: 1,             // Number of GPUs to use\n  gpuStartIndex: 0,      // Starting GPU index (usually 0)\n\n  // Tracking settings\n  numAverages: 1,\n  averageGap: 0.25,\n\n  // Display settings\n  renderPose: 0,\n  outputSize: \"-1x-1\",\n  netInputSize: \"-1x256\",\n\n  // Model settings\n  poseMode: 1,\n  poseModel: \"BODY_25\",\n  alphaPose: 0.5,\n\n  // Advanced settings\n  alphaHeatmap: 0.5,\n  partToShow: 0,\n  renderThreshold: 0.05,\n  fpsMax: -1,\n  upSamplingRatio: 0.0\n);\n</code></pre> <p>Important</p> <p>The <code>modelFolder</code> parameter must point to your OpenPose models directory. This is usually found in the OpenPose installation directory.</p> <p>Tip</p> <p>If you experience performance issues, try reducing the <code>netInputSize</code> parameter to a smaller size like \"-1x128\" for faster processing, though this may reduce tracking accuracy.</p>"},{"location":"getting-started/#step-4-start-tracking","title":"Step 4: Start Tracking","text":"<p>Once configured, you can start tracking:</p> <pre><code>Kinect.startTracking;\n</code></pre> <p>If you want to see what the Kinect is seeing:</p> <pre><code>Kinect.showDisplay;  // Show the camera view with pose overlay\n// Later, to hide it:\nKinect.hideDisplay;\n</code></pre>"},{"location":"getting-started/#step-5-use-joint-positions-in-your-sound","title":"Step 5: Use Joint Positions in Your Sound","text":"<p>Now you can use the <code>Kinect.kr</code> UGen to map joint positions to sound parameters:</p> <pre><code>// Create a SynthDef that uses Kinect data\nSynthDef(\\kinect_controlled_synth, {\n  // Map right wrist Y position (up/down) to frequency\n  var freq = Kinect.kr(200, 800, \"RWrist\", \"Y\");\n\n  // Map left wrist X position (left/right) to amplitude\n  var amp = Kinect.kr(0, 1, \"LWrist\", \"X\");\n\n  // Create a sound using these parameters\n  var sound = SinOsc.ar(freq) * amp;\n\n  // Output the sound\n  Out.ar(0, sound ! 2);\n}).add;\n\n// Create an instance of the synth\nx = Synth(\\kinect_controlled_synth);\n\n// Later, to stop it:\nx.free;\n</code></pre>"},{"location":"getting-started/#understanding-kinectkr-parameters","title":"Understanding Kinect.kr Parameters","text":"<p>The <code>Kinect.kr</code> UGen has the following parameters:</p> <pre><code>Kinect.kr(minval, maxval, joint_name, joint_coordinate)\n</code></pre> <ul> <li><code>minval</code>: The minimum value in the output range</li> <li><code>maxval</code>: The maximum value in the output range</li> <li><code>joint_name</code>: The name of the joint to track (see list below)</li> <li><code>joint_coordinate</code>: Either \"X\" (horizontal) or \"Y\" (vertical)</li> </ul>"},{"location":"getting-started/#available-joints","title":"Available Joints","text":"<p>SCKinect can track the following joints (using the BODY_25 model):</p> <ul> <li><code>\"Nose\"</code> - Top of the nose</li> <li><code>\"Neck\"</code> - Base of the neck</li> <li><code>\"RShoulder\"</code> - Right shoulder</li> <li><code>\"RElbow\"</code> - Right elbow</li> <li><code>\"RWrist\"</code> - Right wrist</li> <li><code>\"LShoulder\"</code> - Left shoulder</li> <li><code>\"LElbow\"</code> - Left elbow</li> <li><code>\"LWrist\"</code> - Left wrist</li> <li><code>\"MidHip\"</code> - Middle of the hips</li> <li><code>\"RHip\"</code> - Right hip</li> <li><code>\"RKnee\"</code> - Right knee</li> <li><code>\"RAnkle\"</code> - Right ankle</li> <li><code>\"LHip\"</code> - Left hip</li> <li><code>\"LKnee\"</code> - Left knee</li> <li><code>\"LAnkle\"</code> - Left ankle</li> <li><code>\"REye\"</code> - Right eye</li> <li><code>\"LEye\"</code> - Left eye</li> <li><code>\"REar\"</code> - Right ear</li> <li><code>\"LEar\"</code> - Left ear</li> <li><code>\"LBigToe\"</code> - Left big toe</li> <li><code>\"LSmallToe\"</code> - Left small toe</li> <li><code>\"LHeel\"</code> - Left heel</li> <li><code>\"RBigToe\"</code> - Right big toe</li> <li><code>\"RSmallToe\"</code> - Right small toe</li> <li><code>\"RHeel\"</code> - Right heel</li> </ul>"},{"location":"getting-started/#step-6-cleanup","title":"Step 6: Cleanup","text":"<p>When you're done using the Kinect:</p> <pre><code>// Stop tracking\nKinect.stopTracking;\n\n// Stop the device\nKinect.stop;\n\n// Close the device\nKinect.closeDevice(\"YOUR_DEVICE_SERIAL\");\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you understand the basics, check out the Wind Example for a more complete implementation. You can also explore the API Reference for more detailed information about all available functions. </p>"},{"location":"installation/","title":"Installation Guide","text":"<p>Installing SCKinect requires setting up several dependencies before building the plugin. This guide will walk you through each step of the process.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<p>Before installing SCKinect, make sure you have the following:</p> <ul> <li>A Kinect v2 sensor device</li> <li>CUDA-capable NVIDIA GPU (required for effective performance)</li> <li>CMake (version 3.5 or higher)</li> <li>C++ compiler with C++17 support (gcc, clang, or MSVC)</li> <li>SuperCollider (with source code)</li> <li>libfreenect2</li> <li>OpenPose with CUDA support</li> </ul> <p>Hardware Requirements</p> <p>The current implementation requires CUDA for effective real-time body tracking. The CPU-only implementation is in progress but not yet fully functional for real-time applications. For best results, you need:</p> <ul> <li>NVIDIA GPU with CUDA capability 3.0 or higher</li> <li>At least 2GB of GPU memory (4GB+ recommended)</li> <li>Recent NVIDIA drivers</li> </ul>"},{"location":"installation/#step-1-install-supercollider","title":"Step 1: Install SuperCollider","text":"<p>If you don't already have SuperCollider installed:</p> <ol> <li>Download and install from the SuperCollider website</li> <li>For building SCKinect, you also need the SuperCollider source code:</li> </ol> <pre><code>git clone https://github.com/supercollider/supercollider.git\n</code></pre>"},{"location":"installation/#step-2-install-cuda","title":"Step 2: Install CUDA","text":"<p>SCKinect requires CUDA for effective performance:</p> <ol> <li>Download and install the CUDA Toolkit from NVIDIA's website</li> <li>Make sure to install the driver that comes with the toolkit</li> <li>Verify the installation by running:</li> </ol> <pre><code>nvcc --version\n</code></pre>"},{"location":"installation/#step-3-install-libfreenect2","title":"Step 3: Install libfreenect2","text":"<p>libfreenect2 is the library that enables communication with the Kinect device.</p> <pre><code>git clone https://github.com/OpenKinect/libfreenect2.git\ncd libfreenect2\nmkdir build &amp;&amp; cd build\ncmake .. -DCUDA_PROPAGATE_HOST_FLAGS=off -DENABLE_CUDA=ON\nmake\nsudo make install\n</code></pre> <p>For detailed installation instructions and troubleshooting, see the libfreenect2 installation guide.</p>"},{"location":"installation/#step-4-install-openpose-with-cuda-support","title":"Step 4: Install OpenPose with CUDA Support","text":"<p>OpenPose is required for body tracking. Ensure you build it with CUDA support:</p> <pre><code>git clone https://github.com/CMU-Perceptual-Computing-Lab/openpose.git\ncd openpose\nmkdir build &amp;&amp; cd build\ncmake .. -DGPU_MODE=CUDA -DBUILD_PYTHON=OFF\nmake -j`nproc`\nsudo make install\n</code></pre> <p>For detailed installation instructions and troubleshooting, see the OpenPose installation guide.</p> <p>Important</p> <p>Remember the location where you installed OpenPose, as you'll need to reference the models directory when configuring SCKinect.</p>"},{"location":"installation/#step-5-build-and-install-sckinect","title":"Step 5: Build and Install SCKinect","text":"<p>Now you can build the SCKinect plugin:</p> <pre><code>git clone https://github.com/L42i/SCKinect.git\ncd SCKinect\nmkdir build &amp;&amp; cd build\n</code></pre> <p>Configure the build with CMake:</p> <pre><code>cmake .. -DCMAKE_BUILD_TYPE=Release -DSC_PATH=/path/to/supercollider/source\n</code></pre> <p>Or to install directly to your SuperCollider extensions directory:</p> <pre><code>cmake .. -DCMAKE_BUILD_TYPE=Release -DSC_PATH=/path/to/supercollider/source -DCMAKE_INSTALL_PREFIX=/path/to/extensions\n</code></pre> <p>Build and install:</p> <pre><code>cmake --build . --config Release\ncmake --build . --config Release --target install\n</code></pre>"},{"location":"installation/#step-6-verify-installation","title":"Step 6: Verify Installation","text":"<ol> <li>Connect your Kinect sensor to your computer</li> <li>Start SuperCollider and run:</li> </ol> <pre><code>Kinect.findAvailable;\n</code></pre> <p>This should list your connected Kinect device with its serial number.</p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#common-issues","title":"Common Issues","text":"<ol> <li>Kinect not detected</li> <li>Make sure your Kinect is properly connected</li> <li> <p>Check if libfreenect2 can detect the device using the libfreenect2 examples</p> </li> <li> <p>Build errors</p> </li> <li>Ensure all dependencies are properly installed</li> <li>Check that your compiler supports C++17</li> <li> <p>Verify path to SuperCollider source is correct</p> </li> <li> <p>OpenPose errors</p> </li> <li> <p>Make sure the path to OpenPose models is correctly specified when calling <code>Kinect.configureTracking()</code></p> </li> <li> <p>Performance issues</p> </li> <li>Ensure you are using the \"CUDAKDE\" or \"CUDA\" pipeline for best performance</li> <li>The \"CPU\" pipeline is still in development and may not work effectively</li> <li>Check your GPU meets the minimum requirements</li> </ol>"},{"location":"installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here, check: - The GitHub repository for open issues - SuperCollider forum for community support </p>"},{"location":"api/cpp-interface/","title":"C++ Interface","text":"<p>The SCKinect plugin is implemented in C++ and provides a bridge between the Kinect sensor, OpenPose, and SuperCollider. This page documents the C++ components and how they interact with each other.</p> <p>CUDA Requirement</p> <p>The current implementation requires CUDA for effective real-time performance. The CPU-only implementation is in progress but not yet fully functional. The C++ code assumes CUDA availability for optimal OpenPose operation.</p>"},{"location":"api/cpp-interface/#key-classes-and-structures","title":"Key Classes and Structures","text":""},{"location":"api/cpp-interface/#kinect-ugen-class","title":"Kinect UGen Class","text":"<pre><code>namespace Kinect {\nclass Kinect : public SCUnit {\npublic:\n    Kinect();\nprivate:\n    // Calc function\n    void next(int nSamples);\n    // Member variables\n};\n}\n</code></pre> <p>This is the main UGen class that SuperCollider interfaces with. It:</p> <ul> <li>Handles the audio rate processing in the <code>next</code> method</li> <li>Maps joint positions to output values</li> <li>Provides the interface for SuperCollider's <code>Kinect.kr</code> method</li> </ul>"},{"location":"api/cpp-interface/#kinectdata-structure","title":"KinectData Structure","text":"<pre><code>struct KinectData {\n    op::WrapperT&lt;op::Datum&gt; opWrapperT; // OpenPose wrapper\n    std::shared_ptr&lt;WUserInput&gt; wUserInput;\n    std::shared_ptr&lt;WUserOutput&gt; wUserOutput;\n    libfreenect2::Freenect2 mFreenect2;\n    libfreenect2::PacketPipeline* mPipeline = new libfreenect2::CpuPacketPipeline();\n    std::unordered_map&lt;std::string, libfreenect2::Freenect2Device*&gt; mOpenDevices;\n    std::string mSelectedSerial;\n    PacketPipeline selectedPipeline = CPU;\n};\n</code></pre> <p>This structure maintains the state of the Kinect connection and OpenPose processing. It contains:</p> <ul> <li>The OpenPose wrapper</li> <li>Input and output handlers for the Kinect frames</li> <li>Libfreenect2 instances for Kinect communication</li> <li>Maps of connected devices</li> <li>Pipeline configuration</li> </ul> <p>While the default pipeline is CPU, this is overridden in practice to use CUDA for effective performance.</p>"},{"location":"api/cpp-interface/#wuserinput-class","title":"WUserInput Class","text":"<pre><code>class WUserInput : public op::WorkerProducer&lt;std::shared_ptr&lt;std::vector&lt;std::shared_ptr&lt;op::Datum&gt;&gt;&gt;&gt; {\npublic:\n    WUserInput();\n    ~WUserInput();\n    void setDevice(libfreenect2::Freenect2Device* device);\n    void initializationOnThread();\n    std::shared_ptr&lt;std::vector&lt;std::shared_ptr&lt;op::Datum&gt;&gt;&gt; workProducer();\nprivate:\n    libfreenect2::Freenect2Device* mDev;\n    libfreenect2::SyncMultiFrameListener mListener;\n    libfreenect2::FrameMap mFrames;\n};\n</code></pre> <p>This class is responsible for acquiring frames from the Kinect and feeding them to OpenPose. See User Input for more details.</p>"},{"location":"api/cpp-interface/#wuseroutput-class","title":"WUserOutput Class","text":"<pre><code>class WUserOutput : public op::WorkerConsumer&lt;std::shared_ptr&lt;std::vector&lt;std::shared_ptr&lt;op::Datum&gt;&gt;&gt;&gt; {\npublic:\n    WUserOutput();\n    ~WUserOutput();\n    void initializationOnThread();\n    void workConsumer(const std::shared_ptr&lt;std::vector&lt;std::shared_ptr&lt;op::Datum&gt;&gt;&gt;&amp; datumsPtr);\n    op::Array&lt;float&gt; poseKeypoints;\n    bool isDisplayEnabled = true;\n};\n</code></pre> <p>This class processes the output from OpenPose and makes the pose data available to the Kinect UGen. See User Output for more details.</p>"},{"location":"api/cpp-interface/#enumerations","title":"Enumerations","text":""},{"location":"api/cpp-interface/#packetpipeline-enum","title":"PacketPipeline Enum","text":"<pre><code>enum PacketPipeline {\n    Dump, CPU, OpenGL, CUDA, CUDAKDE\n};\n</code></pre> <p>This enumeration defines the different processing pipelines available for the Kinect sensor:</p> <ul> <li>Dump: Minimal processing (for testing)</li> <li>CPU: CPU-based processing (slower, still in development)</li> <li>OpenGL: OpenGL-accelerated processing</li> <li>CUDA: CUDA GPU-accelerated processing  </li> <li>CUDAKDE: Enhanced CUDA processing with Kernel Density Estimation filtering (recommended)</li> </ul> <p>Note</p> <p>While all these pipelines are defined, the current implementation works best with CUDA and CUDAKDE options. The CPU pipeline is still in development and may not provide adequate performance for real-time applications.</p>"},{"location":"api/cpp-interface/#jointname-enum","title":"JointName Enum","text":"<pre><code>enum JointName {\n    Nose, Neck, RShoulder, RElbow, RWrist,\n    LShoulder, LElbow, LWrist, MidHip, RHip,\n    RKnee, RAnkle, LHip, LKnee, LAnkle, REye,\n    LEye, REar, LEar, LBigToe, LSmallToe, LHeel,\n    RBigToe, RSmallToe, RHeel\n};\n</code></pre> <p>This enumeration defines the indices for different body joints that can be tracked.</p>"},{"location":"api/cpp-interface/#jointcoordinate-enum","title":"JointCoordinate Enum","text":"<pre><code>enum JointCoordinate {\n    X, Y\n};\n</code></pre> <p>This enumeration defines the available coordinates for joint positions.</p>"},{"location":"api/cpp-interface/#command-handlers","title":"Command Handlers","text":"<p>The C++ interface exposes several command handlers that are called when SuperCollider sends commands to the plugin:</p>"},{"location":"api/cpp-interface/#pipeline-commands","title":"Pipeline Commands","text":"<pre><code>bool KinectCmd_setPipeline2(World* world, void* inUserData)\nvoid KinectCmd_setPipeline(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\n</code></pre> <p>These functions handle the <code>setPipeline</code> command from SuperCollider, configuring the appropriate libfreenect2 pipeline. The implementation creates the appropriate pipeline object based on the selected type:</p> <pre><code>// Pseudocode from implementation\nswitch(pipeline) {\n    case CUDA:\n        kinectData-&gt;mPipeline = new libfreenect2::CudaPacketPipeline();\n        break;\n    case CUDAKDE:\n        kinectData-&gt;mPipeline = new libfreenect2::CudaKdePacketPipeline();\n        break;\n    // other cases...\n}\n</code></pre>"},{"location":"api/cpp-interface/#device-commands","title":"Device Commands","text":"<pre><code>void KinectCmd_findAvailable(World* world, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\nbool KinectCmd_openDevice2(World* world, void* inUserData)\nvoid KinectCmd_openDevice(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\nvoid KinectCmd_closeDevice(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\n</code></pre> <p>These functions handle device discovery and connection, allowing SuperCollider to find and connect to Kinect devices.</p>"},{"location":"api/cpp-interface/#tracking-commands","title":"Tracking Commands","text":"<pre><code>void KinectCmd_start(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\nvoid KinectCmd_stop(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\nvoid KinectCmd_configureTracking(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\nvoid KinectCmd_startTracking(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\nvoid KinectCmd_stopTracking(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\nvoid KinectCmd_showDisplay(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\nvoid KinectCmd_hideDisplay(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\n</code></pre> <p>These functions handle the various tracking commands, configuring and controlling the OpenPose tracking process. The <code>configureTracking</code> command sets up the OpenPose wrapper with GPU configuration:</p> <pre><code>// Pseudocode from implementation\nwrapperStructOutput.gpuNumber = numGpu;\nwrapperStructOutput.gpuStart = gpuStartIndex;\n// ...other configuration\n</code></pre>"},{"location":"api/cpp-interface/#ugen-processing","title":"UGen Processing","text":"<pre><code>void Kinect::next(int nSamples) {\n    // Get the input parameters\n    float minval = in0(0);\n    float maxval = in0(1);\n    int joint = in0(2);\n    int coord = in0(3);\n\n    // Access poseKeypoints from WUserOutput\n    // Map joint position to output range\n    // Write output\n}\n</code></pre> <p>The <code>next</code> method is called at the control rate to process and output the joint position data. It:</p> <ol> <li>Reads the input parameters (min/max range, joint and coordinate indices)</li> <li>Accesses the current pose data from the <code>WUserOutput</code></li> <li>Maps the raw coordinate values to the specified output range</li> <li>Writes the result to the output buffer</li> </ol>"},{"location":"api/cpp-interface/#plugin-registration","title":"Plugin Registration","text":"<pre><code>PluginLoad(KinectUGens) {\n    // Register the UGen\n    registerUnit&lt;Kinect::Kinect&gt;(\"Kinect\");\n\n    // Register command handlers\n    DefinePlugInCmd(\"setPipeline\", KinectCmd_setPipeline, KinectCmd_setPipeline2);\n    DefinePlugInCmd(\"findAvailable\", KinectCmd_findAvailable);\n    // ... additional command registrations\n}\n</code></pre> <p>This section registers the Kinect UGen and all its command handlers with SuperCollider, making them available in the language.</p>"},{"location":"api/cpp-interface/#implementation-notes","title":"Implementation Notes","text":""},{"location":"api/cpp-interface/#thread-safety","title":"Thread Safety","text":"<p>The plugin uses a thread-safe design to handle multiple concurrent operations:</p> <ul> <li>The UGen's <code>next</code> method runs in the audio thread</li> <li>The OpenPose processing runs in a separate thread</li> <li>Communication between threads is managed through thread-safe data structures</li> </ul>"},{"location":"api/cpp-interface/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>The <code>CUDAKDE</code> pipeline provides the best performance but requires CUDA-capable GPU</li> <li>Frame rate is affected by the <code>netInputSize</code> parameter in <code>configureTracking</code></li> <li>Smaller image sizes result in faster processing but potentially less accurate tracking</li> <li>The CPU implementation is still in development and not recommended for real-time use</li> </ul>"},{"location":"api/cpp-interface/#hardware-requirements","title":"Hardware Requirements","text":"<p>For optimal performance, the following hardware is recommended:</p> <ul> <li>NVIDIA GPU with CUDA capability 3.0 or higher</li> <li>At least 2GB of GPU memory (4GB+ recommended)</li> <li>Recent NVIDIA drivers</li> <li>USB 3.0 port for the Kinect sensor</li> </ul>"},{"location":"api/cpp-interface/#memory-management","title":"Memory Management","text":"<ul> <li>The plugin uses smart pointers (<code>std::shared_ptr</code>) for safe resource management</li> <li>Device connections are tracked in a map to prevent duplicate connections</li> <li>Resources are properly cleaned up when devices are disconnected </li> </ul>"},{"location":"api/kinect-class/","title":"Kinect Class API Reference","text":"<p>This page documents all methods and properties of the <code>Kinect</code> class in SuperCollider. This class provides the interface between SuperCollider and the Kinect sensor hardware.</p> <p>CUDA Requirement</p> <p>The current implementation requires CUDA for effective real-time performance. The CPU-only implementation is in progress but not yet fully functional. You will need a CUDA-capable NVIDIA GPU to use this plugin effectively.</p>"},{"location":"api/kinect-class/#class-variables","title":"Class Variables","text":"<pre><code>classvar &lt;openDevices, &lt;deviceCount, &lt;packetPipelines, &lt;jointNames, &lt;jointCoordinates;\n</code></pre> <ul> <li>openDevices: Dictionary mapping device serial numbers to device IDs</li> <li>deviceCount: Number of currently connected devices</li> <li>packetPipelines: Dictionary mapping pipeline names to internal IDs</li> <li>jointNames: Dictionary mapping joint names to indices </li> <li>jointCoordinates: Dictionary mapping coordinate names to indices</li> </ul>"},{"location":"api/kinect-class/#ugen-methods","title":"UGen Methods","text":""},{"location":"api/kinect-class/#kr","title":"kr","text":"<pre><code>*kr { |minval=0, maxval=1, joint_name=\"Nose\", joint_coordinate=\"X\"| }\n</code></pre> <p>Creates a control rate UGen that outputs the position of a specific joint.</p> <p>Parameters:</p> <ul> <li>minval (Float): Minimum value of the output range (default: 0)</li> <li>maxval (Float): Maximum value of the output range (default: 1)</li> <li>joint_name (String): Name of the joint to track (default: \"Nose\")</li> <li>joint_coordinate (String): \"X\" or \"Y\" coordinate (default: \"X\")</li> </ul> <p>Returns: A control rate UGen that maps the joint position to the specified range.</p> <p>Example:</p> <pre><code>// Map the Y position of the right wrist (0-1 range)\nvar rightHandHeight = Kinect.kr(0, 1, \"RWrist\", \"Y\");\n\n// Use it to control the frequency of a sine oscillator (200-800 Hz)\nSinOsc.ar(Kinect.kr(200, 800, \"RWrist\", \"Y\"));\n</code></pre>"},{"location":"api/kinect-class/#device-management-methods","title":"Device Management Methods","text":""},{"location":"api/kinect-class/#findavailable","title":"findAvailable","text":"<pre><code>*findAvailable { }\n</code></pre> <p>Scans for connected Kinect devices and prints their serial numbers to the post window.</p> <p>Example:</p> <pre><code>Kinect.findAvailable;\n// Posts something like: Found device with serial: 065915234247\n</code></pre>"},{"location":"api/kinect-class/#setpipeline","title":"setPipeline","text":"<pre><code>*setPipeline { |pipeline = \"CPU\"| }\n</code></pre> <p>Sets the processing pipeline used for Kinect data.</p> <p>Parameters:</p> <ul> <li>pipeline (String): Name of the pipeline to use. Options are:</li> <li>\"Dump\": Minimal processing (for testing)</li> <li>\"CPU\": CPU-based processing (slower, still in development)</li> <li>\"OpenGL\": OpenGL-accelerated processing</li> <li>\"CUDA\": CUDA GPU-accelerated processing</li> <li>\"CUDAKDE\": Enhanced CUDA processing with KDE filtering (recommended)</li> </ul> <p>Important</p> <p>While the \"CPU\" pipeline is available as an option, it is still in development and not recommended for real-time applications. For best performance, use \"CUDAKDE\" or \"CUDA\", which require an NVIDIA GPU with CUDA support.</p> <p>Example:</p> <pre><code>// Use CUDA processing (recommended)\nKinect.setPipeline(\"CUDAKDE\");\n\n// Fallback to CPU processing (not recommended for real-time use)\nKinect.setPipeline(\"CPU\");\n</code></pre>"},{"location":"api/kinect-class/#opendevice","title":"openDevice","text":"<pre><code>*openDevice { |serial| }\n</code></pre> <p>Opens a connection to a specific Kinect device.</p> <p>Parameters:</p> <ul> <li>serial (String): Serial number of the device to open</li> </ul> <p>Example:</p> <pre><code>Kinect.openDevice(\"065915234247\");\n</code></pre>"},{"location":"api/kinect-class/#closedevice","title":"closeDevice","text":"<pre><code>*closeDevice { |serial| }\n</code></pre> <p>Closes the connection to a specific Kinect device.</p> <p>Parameters:</p> <ul> <li>serial (String): Serial number of the device to close</li> </ul> <p>Example:</p> <pre><code>Kinect.closeDevice(\"065915234247\");\n</code></pre>"},{"location":"api/kinect-class/#start","title":"start","text":"<pre><code>*start { }\n</code></pre> <p>Starts data acquisition from the Kinect device.</p> <p>Example:</p> <pre><code>Kinect.start;\n</code></pre>"},{"location":"api/kinect-class/#stop","title":"stop","text":"<pre><code>*stop { }\n</code></pre> <p>Stops data acquisition from the Kinect device.</p> <p>Example:</p> <pre><code>Kinect.stop;\n</code></pre>"},{"location":"api/kinect-class/#tracking-methods","title":"Tracking Methods","text":""},{"location":"api/kinect-class/#configuretracking","title":"configureTracking","text":"<pre><code>*configureTracking {|loggingLevel=3,\nmaxPeople= -1,\nmodelFolder=\"\",\nnumGpu= -1,\ngpuStartIndex=0,\nnumAverages=1,\naverageGap=0.25,\nrenderPose= -1,\noutputSize=\"-1x-1\",\nnetInputSize=\"-1x368\",\nposeMode=1,\nposeModel=\"BODY_25\",\nalphaPose=0.5,\nalphaHeatmap=0.5,\npartToShow=0,\nrenderThreshold=0.05,\nfpsMax= -1,\nupSamplingRatio=0.0| }\n</code></pre> <p>Configures the OpenPose body tracking system.</p> <p>Parameters:</p> <ul> <li>loggingLevel (Integer): Verbosity level (0-3)</li> <li>maxPeople (Integer): Maximum number of people to track (-1 for no limit)</li> <li>modelFolder (String): Path to OpenPose model files</li> <li>numGpu (Integer): Number of GPUs to use (-1 for auto-detect)</li> <li>gpuStartIndex (Integer): Index of the first GPU to use</li> <li>numAverages (Integer): Number of frames to average for smoother tracking</li> <li>averageGap (Float): Time gap between averaged frames (seconds)</li> <li>renderPose (Integer): Whether to render pose overlay (-1 for auto)</li> <li>outputSize (String): Output frame size in \"WxH\" format (-1 for native)</li> <li>netInputSize (String): Network input size in \"WxH\" format</li> <li>poseMode (Integer): Pose detection mode</li> <li>poseModel (String): Model to use (e.g., \"BODY_25\")</li> <li>alphaPose (Float): Alpha (transparency) of pose overlay</li> <li>alphaHeatmap (Float): Alpha of confidence heatmap overlay</li> <li>partToShow (Integer): Which body part to focus on (0 for all)</li> <li>renderThreshold (Float): Minimum confidence for a joint to be rendered</li> <li>fpsMax (Float): Maximum frames per second (-1 for no limit)</li> <li>upSamplingRatio (Float): Sampling ratio for confidence map</li> </ul> <p>Performance Tip</p> <p>For better performance on lower-end GPUs, try setting <code>netInputSize</code> to a smaller value like \"-1x128\" and limit <code>maxPeople</code> to 1 if you're only tracking yourself.</p> <p>Example:</p> <pre><code>Kinect.configureTracking(\n  3, 1, \"/path/to/openpose/models\",\n  1, 0, 1, 0.25,\n  0, \"-1x-1\", \"-1x256\",\n  1, \"BODY_25\", 0.5,\n  0.5, 0, 0.05, -1, 0.0\n);\n</code></pre>"},{"location":"api/kinect-class/#starttracking","title":"startTracking","text":"<pre><code>*startTracking { }\n</code></pre> <p>Starts the body tracking process.</p> <p>Example:</p> <pre><code>Kinect.startTracking;\n</code></pre>"},{"location":"api/kinect-class/#stoptracking","title":"stopTracking","text":"<pre><code>*stopTracking { }\n</code></pre> <p>Stops the body tracking process.</p> <p>Example:</p> <pre><code>Kinect.stopTracking;\n</code></pre>"},{"location":"api/kinect-class/#showdisplay","title":"showDisplay","text":"<pre><code>*showDisplay { }\n</code></pre> <p>Shows the visual display with camera feed and pose overlay.</p> <p>Example:</p> <pre><code>Kinect.showDisplay;\n</code></pre>"},{"location":"api/kinect-class/#hidedisplay","title":"hideDisplay","text":"<pre><code>*hideDisplay { }\n</code></pre> <p>Hides the visual display.</p> <p>Example:</p> <pre><code>Kinect.hideDisplay;\n</code></pre>"},{"location":"api/kinect-class/#available-joints","title":"Available Joints","text":"<p>The following joint names are available for tracking with the \"BODY_25\" pose model:</p> Joint Name Description Joint Name Description \"Nose\" Nose \"REye\" Right eye \"Neck\" Base of neck \"LEye\" Left eye \"RShoulder\" Right shoulder \"REar\" Right ear \"RElbow\" Right elbow \"LEar\" Left ear \"RWrist\" Right wrist \"LBigToe\" Left big toe \"LShoulder\" Left shoulder \"LSmallToe\" Left small toe \"LElbow\" Left elbow \"LHeel\" Left heel \"LWrist\" Left wrist \"RBigToe\" Right big toe \"MidHip\" Middle of hips \"RSmallToe\" Right small toe \"RHip\" Right hip \"RHeel\" Right heel \"RKnee\" Right knee \"RAnkle\" Right ankle \"LHip\" Left hip \"LKnee\" Left knee \"LAnkle\" Left ankle"},{"location":"api/kinect-class/#typical-usage-pattern","title":"Typical Usage Pattern","text":"<pre><code>// Boot the server\ns.boot;\n\n// Find and connect to Kinect\nKinect.findAvailable;\nKinect.setPipeline(\"CUDAKDE\");  // CUDA pipeline is required for real-time performance\nKinect.openDevice(\"YOUR_DEVICE_SERIAL\");\nKinect.start;\n\n// Configure and start tracking\nKinect.configureTracking(3, 1, \"/path/to/openpose/models\");\nKinect.startTracking;\n\n// Create a synth that uses Kinect data\nSynthDef(\\kinect_synth, {\n  var freq = Kinect.kr(200, 800, \"RWrist\", \"Y\");\n  var amp = Kinect.kr(0, 1, \"LWrist\", \"X\");\n  Out.ar(0, SinOsc.ar(freq) * amp ! 2);\n}).add;\n\nx = Synth(\\kinect_synth);\n\n// When done\nx.free;\nKinect.stopTracking;\nKinect.stop;\nKinect.closeDevice(\"YOUR_DEVICE_SERIAL\");\n</code></pre>"},{"location":"components/kinect-ugen/","title":"Kinect UGen","text":"<p>The <code>Kinect</code> UGen is the core of SCKinect, providing real-time access to body tracking data from the Kinect sensor. This component runs as a native SuperCollider UGen (Unit Generator), allowing direct integration with the audio synthesis engine.</p> <p>CUDA Requirement</p> <p>The current implementation requires CUDA for effective real-time performance. The CPU-only implementation is in progress but not yet fully functional. You will need a CUDA-capable NVIDIA GPU to use this UGen effectively.</p>"},{"location":"components/kinect-ugen/#overview","title":"Overview","text":"<p>The Kinect UGen connects to the following components:</p> <ol> <li>libfreenect2 - For raw device communication</li> <li>OpenPose - For body pose estimation (via CUDA)</li> <li>SuperCollider - For audio control signals</li> </ol> <p>It processes data in real-time, tracking up to 25 different body joints and making their positions available as control signals.</p>"},{"location":"components/kinect-ugen/#technical-implementation","title":"Technical Implementation","text":"<p>The UGen consists of:</p> <ul> <li>A C++ class (<code>Kinect.cpp</code>, <code>Kinect.hpp</code>) handling the low-level communication and processing</li> <li>A SuperCollider class (<code>Kinect.sc</code>) providing a convenient interface</li> <li>Supporting classes for input/output handling (<code>WUserInput</code>, <code>WUserOutput</code>)</li> </ul> <p>Under the hood, the UGen:</p> <ol> <li>Receives raw data from the Kinect sensor through libfreenect2</li> <li>Passes this data to OpenPose for body pose detection (using GPU acceleration)</li> <li>Makes the pose data (joint positions) available as SuperCollider control signals</li> <li>Provides methods for configuring and managing the Kinect connection</li> </ol>"},{"location":"components/kinect-ugen/#class-structure","title":"Class Structure","text":""},{"location":"components/kinect-ugen/#supercollider-class","title":"SuperCollider Class","text":"<p>The <code>Kinect</code> class in <code>Kinect.sc</code> provides the SuperCollider interface to the UGen. It includes:</p> <ul> <li>Class variables:</li> <li><code>openDevices</code>: Dictionary of connected devices</li> <li><code>deviceCount</code>: Number of connected devices</li> <li><code>packetPipelines</code>: Available processing pipelines (CPU, CUDA, etc.)</li> <li><code>jointNames</code>: Dictionary mapping joint names to indices</li> <li> <p><code>jointCoordinates</code>: Dictionary mapping coordinate names to indices</p> </li> <li> <p>UGen methods:</p> </li> <li> <p><code>kr</code>: Creates a control rate UGen returning a specific joint coordinate</p> </li> <li> <p>Device management methods:</p> </li> <li><code>findAvailable</code>: Scans for connected Kinect devices</li> <li><code>setPipeline</code>: Selects the processing pipeline (CUDA/CUDAKDE recommended)</li> <li><code>openDevice</code>: Connects to a specific Kinect device</li> <li><code>closeDevice</code>: Disconnects from a device</li> <li> <p><code>start</code>/<code>stop</code>: Controls the Kinect operation</p> </li> <li> <p>Tracking methods:</p> </li> <li><code>configureTracking</code>: Sets up body tracking parameters</li> <li><code>startTracking</code>/<code>stopTracking</code>: Controls the tracking process</li> <li><code>showDisplay</code>/<code>hideDisplay</code>: Controls the visual display</li> </ul>"},{"location":"components/kinect-ugen/#c-implementation","title":"C++ Implementation","text":"<p>The C++ side handles the actual device operation and data processing:</p> <ul> <li>KinectData structure: Maintains the state of connected devices and OpenPose configuration</li> <li>WUserInput class: Handles frame acquisition from the Kinect sensor</li> <li>WUserOutput class: Processes the OpenPose output and makes the data available to SuperCollider</li> <li>Command handlers: Implement the various commands sent from SuperCollider to control the device</li> </ul>"},{"location":"components/kinect-ugen/#ugen-parameters","title":"UGen Parameters","text":""},{"location":"components/kinect-ugen/#kinectkr","title":"Kinect.kr","text":"<pre><code>Kinect.kr(minval, maxval, joint_name, joint_coordinate)\n</code></pre> <ul> <li><code>minval</code> (Float): Minimum value in the output range (default: 0)</li> <li><code>maxval</code> (Float): Maximum value in the output range (default: 1)</li> <li><code>joint_name</code> (String): Name of the joint to track (default: \"Nose\")</li> <li><code>joint_coordinate</code> (String): \"X\" or \"Y\" coordinate (default: \"X\")</li> </ul> <p>The UGen maps the raw joint position (which is in pixel coordinates) to the specified range. This makes it easy to control audio parameters based on body position.</p>"},{"location":"components/kinect-ugen/#configuretracking-parameters","title":"configureTracking Parameters","text":"<p>The <code>configureTracking</code> method accepts a large number of parameters that control the behavior of OpenPose:</p> <pre><code>Kinect.configureTracking(\n  loggingLevel, maxPeople, modelFolder, numGpu,\n  gpuStartIndex, numAverages, averageGap, renderPose,\n  outputSize, netInputSize, poseMode, poseModel,\n  alphaPose, alphaHeatmap, partToShow, renderThreshold,\n  fpsMax, upSamplingRatio\n)\n</code></pre> <p>Key parameters include:</p> <ul> <li><code>loggingLevel</code>: Controls verbosity (0-3, with 3 being most verbose)</li> <li><code>maxPeople</code>: Maximum number of people to track (-1 for no limit)</li> <li><code>modelFolder</code>: Path to OpenPose models</li> <li><code>numGpu</code>/<code>gpuStartIndex</code>: GPU settings for OpenPose (crucial for performance)</li> <li><code>poseModel</code>: Model to use for tracking (default: \"BODY_25\")</li> <li><code>renderThreshold</code>: Minimum confidence for detecting joints</li> </ul> <p>Performance Considerations</p> <ul> <li>The <code>netInputSize</code> parameter significantly affects performance. Smaller sizes (like \"-1x128\") are faster but less accurate.</li> <li>The CUDA and CUDAKDE pipelines require a NVIDIA GPU but provide much better performance than the CPU pipeline.</li> <li>Multi-person tracking (<code>maxPeople</code> &gt; 1) requires more GPU resources.</li> </ul> <p>See the Getting Started guide for more examples of how to use these parameters. </p>"},{"location":"components/user-input/","title":"User Input Component","text":"<p>The <code>WUserInput</code> class is a critical part of SCKinect that handles the acquisition of frames from the Kinect sensor and prepares them for body tracking. This component acts as a bridge between the Kinect hardware (via libfreenect2) and the OpenPose body tracking system.</p>"},{"location":"components/user-input/#overview","title":"Overview","text":"<p><code>WUserInput</code> is derived from the OpenPose <code>WorkerProducer</code> class, which is designed to produce data to be processed by OpenPose. In this case, it produces frames from the Kinect sensor.</p>"},{"location":"components/user-input/#class-definition","title":"Class Definition","text":"<pre><code>// From WUserInput.hpp\nclass WUserInput : public op::WorkerProducer&lt;std::shared_ptr&lt;std::vector&lt;std::shared_ptr&lt;op::Datum&gt;&gt;&gt;&gt;\n{\npublic:\n    WUserInput();\n    ~WUserInput();\n\n    void setDevice(libfreenect2::Freenect2Device* device);\n    void initializationOnThread();\n    std::shared_ptr&lt;std::vector&lt;std::shared_ptr&lt;op::Datum&gt;&gt;&gt; workProducer();\nprivate:\n    libfreenect2::Freenect2Device* mDev;\n    libfreenect2::SyncMultiFrameListener mListener;\n    libfreenect2::FrameMap mFrames;\n};\n</code></pre>"},{"location":"components/user-input/#key-methods","title":"Key Methods","text":""},{"location":"components/user-input/#constructor-and-destructor","title":"Constructor and Destructor","text":"<pre><code>WUserInput();\n~WUserInput();\n</code></pre> <p>The constructor initializes the frame listener, and the destructor ensures proper cleanup of resources.</p>"},{"location":"components/user-input/#setdevice","title":"setDevice","text":"<pre><code>void setDevice(libfreenect2::Freenect2Device* device);\n</code></pre> <p>This method connects the class to a specific Kinect device. The device is provided by the main Kinect UGen after it has been opened.</p>"},{"location":"components/user-input/#initializationonthread","title":"initializationOnThread","text":"<pre><code>void initializationOnThread();\n</code></pre> <p>This method is called when the OpenPose thread starts. It initializes the necessary resources for frame acquisition.</p>"},{"location":"components/user-input/#workproducer","title":"workProducer","text":"<pre><code>std::shared_ptr&lt;std::vector&lt;std::shared_ptr&lt;op::Datum&gt;&gt;&gt; workProducer();\n</code></pre> <p>This is the main processing method that:</p> <ol> <li>Waits for new frames from the Kinect sensor</li> <li>Converts them from the Kinect format to OpenCV format (which OpenPose uses)</li> <li>Creates OpenPose Datum objects with the frames</li> <li>Returns the Datum objects to OpenPose for processing</li> </ol>"},{"location":"components/user-input/#technical-details","title":"Technical Details","text":""},{"location":"components/user-input/#frame-acquisition","title":"Frame Acquisition","text":"<p>The <code>WUserInput</code> class uses a <code>SyncMultiFrameListener</code> to receive frames from the Kinect. This listener can handle multiple types of frames:</p> <ul> <li>Color (RGB) frames: Used for visual display and body detection</li> <li>Depth frames: Can be used for 3D tracking (though not fully implemented in the current version)</li> <li>IR frames: Infrared data (not used in current implementation)</li> </ul>"},{"location":"components/user-input/#frame-conversion","title":"Frame Conversion","text":"<p>The frames from the Kinect are in a custom format provided by libfreenect2. The <code>workProducer</code> method converts these frames to OpenCV <code>cv::Mat</code> format, which is required by OpenPose.</p>"},{"location":"components/user-input/#threading-model","title":"Threading Model","text":"<p><code>WUserInput</code> runs on a separate thread as part of the OpenPose pipeline. This allows frame acquisition to occur in parallel with body tracking, improving performance.</p>"},{"location":"components/user-input/#usage","title":"Usage","text":"<p>The <code>WUserInput</code> class is not meant to be used directly by SuperCollider code. Instead, it is created and managed by the main Kinect UGen. The UGen sets up the OpenPose pipeline and connects the <code>WUserInput</code> to the appropriate Kinect device.</p>"},{"location":"components/user-input/#relationship-to-other-components","title":"Relationship to Other Components","text":"<ul> <li>Kinect UGen: Creates and manages the <code>WUserInput</code></li> <li>WUserOutput: Receives the processed frames after OpenPose has detected body poses</li> <li>libfreenect2: Provides the raw frame data from the Kinect sensor</li> <li>OpenPose: Processes the frames to detect body poses </li> </ul>"},{"location":"components/user-output/","title":"User Output Component","text":"<p>The <code>WUserOutput</code> class is responsible for processing the results of body tracking from OpenPose and making this data available to the Kinect UGen. This component acts as the bridge between the OpenPose body tracking system and SuperCollider.</p>"},{"location":"components/user-output/#overview","title":"Overview","text":"<p><code>WUserOutput</code> is derived from the OpenPose <code>WorkerConsumer</code> class, which is designed to consume data processed by OpenPose. In this case, it consumes the body pose data detected from Kinect frames.</p>"},{"location":"components/user-output/#class-definition","title":"Class Definition","text":"<pre><code>// From WUserOutput.hpp\nclass WUserOutput : public op::WorkerConsumer&lt;std::shared_ptr&lt;std::vector&lt;std::shared_ptr&lt;op::Datum&gt;&gt;&gt;&gt;\n{\npublic:\n    WUserOutput();\n    ~WUserOutput();\n\n    void initializationOnThread();\n    void workConsumer(const std::shared_ptr&lt;std::vector&lt;std::shared_ptr&lt;op::Datum&gt;&gt;&gt;&amp; datumsPtr);\n\n    op::Array&lt;float&gt; poseKeypoints;\n    bool isDisplayEnabled = true;\n};\n</code></pre>"},{"location":"components/user-output/#key-methods","title":"Key Methods","text":""},{"location":"components/user-output/#constructor-and-destructor","title":"Constructor and Destructor","text":"<pre><code>WUserOutput();\n~WUserOutput();\n</code></pre> <p>These methods handle initialization and cleanup of resources used by the class.</p>"},{"location":"components/user-output/#initializationonthread","title":"initializationOnThread","text":"<pre><code>void initializationOnThread();\n</code></pre> <p>This method is called when the OpenPose thread starts. It initializes any necessary resources for processing the output from OpenPose.</p>"},{"location":"components/user-output/#workconsumer","title":"workConsumer","text":"<pre><code>void workConsumer(const std::shared_ptr&lt;std::vector&lt;std::shared_ptr&lt;op::Datum&gt;&gt;&gt;&amp; datumsPtr);\n</code></pre> <p>This is the main processing method that:</p> <ol> <li>Receives processed <code>Datum</code> objects from OpenPose</li> <li>Extracts the body pose keypoints from the data</li> <li>Stores these keypoints for access by the Kinect UGen</li> <li>Optionally displays the processed frames with pose overlays</li> </ol>"},{"location":"components/user-output/#technical-details","title":"Technical Details","text":""},{"location":"components/user-output/#pose-data-storage","title":"Pose Data Storage","text":"<p>The <code>poseKeypoints</code> member variable stores the latest body pose data from OpenPose. This is a 3D array that contains:</p> <ul> <li>Person index: For multiple people tracking (first dimension)</li> <li>Joint index: For different body parts like hands, shoulders, etc. (second dimension)</li> <li>Coordinate values: X, Y coordinates and confidence value (third dimension)</li> </ul> <p>The format is:</p> <pre><code>poseKeypoints[person_idx][joint_idx][0] = X coordinate\nposeKeypoints[person_idx][joint_idx][1] = Y coordinate\nposeKeypoints[person_idx][joint_idx][2] = Confidence (0-1)\n</code></pre>"},{"location":"components/user-output/#visual-display","title":"Visual Display","text":"<p>The <code>isDisplayEnabled</code> flag controls whether the processed frames are displayed in a window. When enabled, this shows the camera view with body pose overlays, which is useful for debugging and visual feedback.</p>"},{"location":"components/user-output/#data-access-pattern","title":"Data Access Pattern","text":"<p>The <code>poseKeypoints</code> variable is accessed by the Kinect UGen to provide real-time pose data to SuperCollider. The UGen maps these raw coordinates to the range specified in the <code>Kinect.kr</code> method calls.</p>"},{"location":"components/user-output/#threading-model","title":"Threading Model","text":"<p>Like <code>WUserInput</code>, the <code>WUserOutput</code> class runs on a separate thread as part of the OpenPose pipeline. This allows pose data processing to occur in parallel with audio synthesis in SuperCollider.</p>"},{"location":"components/user-output/#usage","title":"Usage","text":"<p>The <code>WUserOutput</code> class is not meant to be used directly by SuperCollider code. Instead, it is created and managed by the main Kinect UGen. The UGen sets up the OpenPose pipeline and regularly accesses the pose data from the <code>WUserOutput</code> instance.</p>"},{"location":"components/user-output/#relationship-to-other-components","title":"Relationship to Other Components","text":"<ul> <li>Kinect UGen: Creates and manages the <code>WUserOutput</code>, accesses its <code>poseKeypoints</code> data</li> <li>WUserInput: Provides the input frames that are eventually processed and passed to <code>WUserOutput</code></li> <li>OpenPose: Processes the frames to detect body poses, passing results to <code>WUserOutput</code></li> <li>SuperCollider: Receives the processed pose data through the Kinect UGen </li> </ul>"},{"location":"examples/wind/","title":"Wind Sound Example","text":"<p>This example demonstrates how to use SCKinect to control a wind-like sound using hand movements. The right hand controls the volume (moving up/down) while the left hand controls the tone (moving left/right).</p> <p>CUDA Requirement</p> <p>This example requires CUDA for effective real-time performance. The CPU-only implementation is in progress but not yet fully functional. You will need a CUDA-capable NVIDIA GPU to run this example as intended.</p>"},{"location":"examples/wind/#overview","title":"Overview","text":"<p>The example creates a simulated wind sound using filtered noise generators. Body tracking data from the Kinect is mapped to control parameters:</p> <ul> <li>Right Wrist Y Position: Controls the amplitude of the wind sound</li> <li>Left Wrist X Position: Controls the cutoff frequency of a lowpass filter</li> </ul> <p>This creates an intuitive and physical way to \"conduct\" the wind sound, making it stronger or weaker, brighter or darker through natural hand movements.</p>"},{"location":"examples/wind/#full-example-code","title":"Full Example Code","text":"<pre><code>s.options.numOutputBusChannels = 6;\ns.waitForBoot({\n    // Setup settings for a Kinect UGen\n    Kinect.findAvailable;\n    Kinect.setPipeline(\"CUDAKDE\");  // CUDA pipeline is required for real-time performance\n    Kinect.openDevice(\"065915234247\");\n    s.sync;\n    Kinect.start;\n    Kinect.configureTracking(\n        3, -1, \"/home/evanmurray/openpose/models\", // Point this to your own models folder.\n        -1, 0, 1, 0.25,\n        -1, \"-1x-1\", \"-1x256\",\n        1, \"BODY_25\", 0.5,\n        0.5, 0, 0.05, -1, 0.0\n    );\n    Kinect.startTracking;\n\n    // Define a SynthDef for a wind sound and send it to the server\n    SynthDef(\\wind_sound, {\n        |out = 0, amplitude = 1, cutoff_frequency = 20000|\n\n        var x = BLowPass4.ar(in: Klank.ar(`[[200, 671, 1153, 1723], nil, [1, 1, 1, 1]],\n            PinkNoise.ar(0.004)*amplitude), freq: cutoff_frequency, rq: 0.1, mul: 0.8, add: 0.0);\n        var y = BLowPass4.ar(in: BrownNoise.ar(0.2) * amplitude, freq: cutoff_frequency, rq: 0.07, mul: 1.0, add: 0.0);\n\n        Out.ar(out, x);\n        Out.ar(out+1, y);\n        Out.ar(out+2, x);\n        Out.ar(out+3, y);\n        Out.ar(out+4, x);\n        Out.ar(out+5, y);\n    }).send(s);\n\n    // Define some control buses for the Kinect\n    ~kinectAmplitudeBus = Bus.control(s, 1);\n    ~kinectFrequencyBus = Bus.control(s, 1);\n\n    // Write the y position of the right wrist detected by the kinect to a control bus to control the amplitude\n    {Out.kr(~kinectAmplitudeBus.index, Kinect.kr(0, 0.5, \"RWrist\", \"Y\"))}.play;\n\n    // Now write the x position of the left wrist info to a frequency bus to control the lowpass filter\n    {Out.kr(~kinectFrequencyBus.index, Kinect.kr(20, 10000, \"LWrist\", \"X\"))}.play;\n\n    // Scope the first control bus to see what the output looks like\n    s.scope(1, 0, rate: 'control');\n\n    // Wait for the control buses to be created before proceeding\n    s.sync;\n\n    // Create a wind sound and send it to speaker #1\n    ~windSound = Synth(\\wind_sound, [\\out, 0, \\amplitude, 0.5, \\cutoff_frequency, 20000]);\n\n    // Map the control buses to the amplitude and frequency\n    ~windSound.map(\\amplitude, ~kinectAmplitudeBus);\n    ~windSound.map(\\cutoff_frequency, ~kinectFrequencyBus);\n\n    s.meter;\n})\nKinect.stopTracking;\nKinect.stop;\nKinect.closeDevice(\"065915234247\");\n</code></pre>"},{"location":"examples/wind/#code-breakdown","title":"Code Breakdown","text":"<p>Let's break down the example into key sections:</p>"},{"location":"examples/wind/#1-server-configuration","title":"1. Server Configuration","text":"<pre><code>s.options.numOutputBusChannels = 6;\ns.waitForBoot({\n    // Rest of the code...\n})\n</code></pre> <p>This configures the SuperCollider server to use 6 output channels (for surround sound) and waits for the server to boot before executing the rest of the code.</p>"},{"location":"examples/wind/#2-kinect-setup","title":"2. Kinect Setup","text":"<pre><code>// Setup settings for a Kinect UGen\nKinect.findAvailable;\nKinect.setPipeline(\"CUDAKDE\");  // CUDA pipeline is required for real-time performance\nKinect.openDevice(\"065915234247\");\ns.sync;\nKinect.start;\n</code></pre> <p>This section: 1. Searches for available Kinect devices 2. Sets the processing pipeline to CUDAKDE (GPU-accelerated, required for real-time tracking) 3. Opens a specific Kinect device (you'll need to replace the serial number with your own) 4. Synchronizes with the server 5. Starts the Kinect data acquisition</p> <p>Note</p> <p>Note the use of \"CUDAKDE\" pipeline, which requires a CUDA-capable GPU. The CPU pipeline is not recommended for this example as it may not provide adequate performance.</p>"},{"location":"examples/wind/#3-tracking-configuration","title":"3. Tracking Configuration","text":"<pre><code>Kinect.configureTracking(\n    3, -1, \"/home/evanmurray/openpose/models\", // Point this to your own models folder.\n    -1, 0, 1, 0.25,\n    -1, \"-1x-1\", \"-1x256\",\n    1, \"BODY_25\", 0.5,\n    0.5, 0, 0.05, -1, 0.0\n);\nKinect.startTracking;\n</code></pre> <p>This configures OpenPose for body tracking: 1. Sets logging level to 3 (verbose) 2. Tracks unlimited number of people (-1) 3. Points to the OpenPose models folder (you'll need to change this path) 4. Uses default GPU settings 5. Sets up tracking parameters 6. Starts the tracking process</p>"},{"location":"examples/wind/#4-wind-sound-synthesis","title":"4. Wind Sound Synthesis","text":"<pre><code>SynthDef(\\wind_sound, {\n    |out = 0, amplitude = 1, cutoff_frequency = 20000|\n\n    var x = BLowPass4.ar(in: Klank.ar(`[[200, 671, 1153, 1723], nil, [1, 1, 1, 1]],\n        PinkNoise.ar(0.004)*amplitude), freq: cutoff_frequency, rq: 0.1, mul: 0.8, add: 0.0);\n    var y = BLowPass4.ar(in: BrownNoise.ar(0.2) * amplitude, freq: cutoff_frequency, rq: 0.07, mul: 1.0, add: 0.0);\n\n    Out.ar(out, x);\n    Out.ar(out+1, y);\n    Out.ar(out+2, x);\n    Out.ar(out+3, y);\n    Out.ar(out+4, x);\n    Out.ar(out+5, y);\n}).send(s);\n</code></pre> <p>This defines a synthesizer for wind sounds: 1. Creates two types of filtered noise:    - <code>x</code>: Resonant filtered pink noise (for tonal wind components)    - <code>y</code>: Lowpass filtered brown noise (for background wind) 2. Both sounds are controlled by the same parameters:    - <code>amplitude</code>: Overall volume    - <code>cutoff_frequency</code>: Brightness/darkness of the sound 3. Outputs the sounds to all 6 channels</p>"},{"location":"examples/wind/#5-control-bus-setup","title":"5. Control Bus Setup","text":"<pre><code>// Define some control buses for the Kinect\n~kinectAmplitudeBus = Bus.control(s, 1);\n~kinectFrequencyBus = Bus.control(s, 1);\n\n// Write the y position of the right wrist detected by the kinect to a control bus to control the amplitude\n{Out.kr(~kinectAmplitudeBus.index, Kinect.kr(0, 0.5, \"RWrist\", \"Y\"))}.play;\n\n// Now write the x position of the left wrist info to a frequency bus to control the lowpass filter\n{Out.kr(~kinectFrequencyBus.index, Kinect.kr(20, 10000, \"LWrist\", \"X\"))}.play;\n</code></pre> <p>This sets up control buses to route Kinect data to the synth: 1. Creates two control buses for amplitude and frequency 2. Maps right wrist Y position (up/down) to amplitude (range 0-0.5) 3. Maps left wrist X position (left/right) to filter cutoff frequency (range 20-10000 Hz)</p>"},{"location":"examples/wind/#6-creating-and-mapping-the-synth","title":"6. Creating and Mapping the Synth","text":"<pre><code>// Create a wind sound and send it to speaker #1\n~windSound = Synth(\\wind_sound, [\\out, 0, \\amplitude, 0.5, \\cutoff_frequency, 20000]);\n\n// Map the control buses to the amplitude and frequency\n~windSound.map(\\amplitude, ~kinectAmplitudeBus);\n~windSound.map(\\cutoff_frequency, ~kinectFrequencyBus);\n</code></pre> <p>This creates the synth and maps the control buses: 1. Creates a wind sound synth with default parameters 2. Maps the amplitude and cutoff_frequency parameters to the control buses</p>"},{"location":"examples/wind/#7-cleanup","title":"7. Cleanup","text":"<pre><code>Kinect.stopTracking;\nKinect.stop;\nKinect.closeDevice(\"065915234247\");\n</code></pre> <p>This stops tracking, stops the Kinect, and closes the device when you're done.</p>"},{"location":"examples/wind/#running-the-example","title":"Running the Example","text":"<p>To run this example:</p> <ol> <li>Replace the serial number (<code>\"065915234247\"</code>) with your own Kinect's serial number</li> <li>Update the path to OpenPose models to match your installation</li> <li>Make sure you have a CUDA-capable GPU installed and properly configured</li> <li>Run the code in SuperCollider</li> <li>Stand in front of the Kinect and move your hands to control the wind sound:</li> <li>Move your right hand up and down to control volume</li> <li>Move your left hand left and right to control the tone</li> </ol> <p>Performance Considerations</p> <p>If you experience lag or stuttering in the tracking: - Try reducing the <code>netInputSize</code> parameter in <code>configureTracking()</code> (e.g., \"-1x128\") - Ensure you're using the \"CUDAKDE\" pipeline - Make sure other GPU-intensive applications are closed - Consider upgrading your GPU if problems persist</p>"},{"location":"examples/wind/#extending-the-example","title":"Extending the Example","text":"<p>You can extend this example in several ways:</p> <ol> <li>Map different body joints to different parameters</li> <li>Add more sound parameters (like reverb, panning, or modulation)</li> <li>Create more complex conditional responses (e.g., specific gestures trigger events)</li> <li>Map multiple people's movements to different instruments </li> </ol>"}]}