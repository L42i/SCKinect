{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SCKinect: Motion Tracking for Sound Generation","text":"<p>SCKinect is a SuperCollider plugin that bridges physical movement with sound generation. It allows you to use a Kinect sensor to track human body movements and map them to sound parameters in SuperCollider.</p> <p>CUDA Requirement</p> <p>The current implementation requires CUDA for effective real-time performance. The CPU-only implementation is in progress but not yet fully functional. You will need a CUDA-capable NVIDIA GPU to use this plugin as described in this guide. This is because in order for the current pose estimation model (OpenPose) to achieve efficient performance, you need to GPU accelerate it unless you have a CPU which can process ridiculous amounts of data like a GPU (some CPU's do have capabilities like that and separate cores for doing this processing i.e. Apple Silicon). But for building OpenPose, it is easiest to build it with CUDA. For this reason, alternative methods other than using OpenPose for CPU users are being explored.</p>"},{"location":"#what-is-sckinect","title":"What is SCKinect?","text":"<p>SCKinect integrates multiple technologies to create an interactive system where physical movements can control sound:</p> <ol> <li>Kinect v2 Sensor - A camera device originally designed for Xbox that can track movements and depth.</li> <li>Libfreenect2 - A tool that grabs raw data from the Kinect sensor and makes it available to your computer. It handles the direct communication with the hardware.</li> <li>OpenPose - Computer vision software that can detect human body poses from camera images.</li> <li>SuperCollider - A platform for audio synthesis and algorithmic composition.</li> </ol> <p>The plugin enables you to map specific body joint positions (like hands, knees, or head) to sound parameters, creating an intuitive and embodied way to generate and control sound.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Track up to 25 body joints in real-time</li> <li>Map any joint's X/Y position to sound parameters</li> <li>Choose from different processing pipelines for libfreenect2 (CPU, OpenGL, CUDA, and CUDAKDE)</li> <li>Multiple configuration options for tracking quality and performance</li> <li>Simple SuperCollider interface for easy integration with your sound design</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<p>See the Installation page for instructions on installing SCKinect. </p>"},{"location":"getting-started/","title":"Getting Started with SCKinect","text":"<p>This guide will walk you through the basic steps of setting up and using SCKinect to connect body movement with sound generation.</p>"},{"location":"getting-started/#basic-workflow","title":"Basic Workflow","text":"<p>Working with SCKinect follows this general workflow:</p> <ol> <li>Boot The Server: Boot the SuperCollider server (scsynth)</li> <li>Setup: Find and connect to the Kinect device</li> <li>Configure: Set tracking parameters for OpenPose</li> <li>Track: Start body tracking</li> <li>Map: Map joint positions to sound parameters</li> <li>Cleanup: Stop tracking and close the device when done</li> </ol>"},{"location":"getting-started/#step-1-setting-up-the-kinect","title":"Step 1: Setting Up the Kinect","text":"<p>First, make sure your Kinect is connected to your computer. Then, in SuperCollider:</p> <pre><code>// Boot the server if it's not already running\ns.boot;\n\n// Find available Kinect devices\nKinect.findAvailable;\n</code></pre> <p>This will print the serial number(s) of any connected Kinect devices to the SuperCollider post window.</p>"},{"location":"getting-started/#step-2-connect-to-the-kinect","title":"Step 2: Connect to the Kinect","text":"<p>Once you have the serial number, you can connect to the device:</p> <pre><code>// Choose a processing pipeline (CUDA is recommended)\nKinect.setPipeline(\"CUDAKDE\");  // \"CUDA\" is also an option but \"CUDAKDE\" looks nice\n\n// Open the device using its serial number\nKinect.openDevice(\"YOUR_DEVICE_SERIAL\");\n\n// Start the device\nKinect.start;\n</code></pre> <p>Note</p> <p>Replace <code>\"YOUR_DEVICE_SERIAL\"</code> with the actual serial number of your Kinect device.</p>"},{"location":"getting-started/#step-3-configure-body-tracking","title":"Step 3: Configure Body Tracking","text":"<p>Before you can start tracking, you need to configure the tracking parameters:</p> <pre><code>Kinect.configureTracking(\n  // Basic settings\n  loggingLevel: 3,\n  maxPeople: 1,\n  modelFolder: \"/path/to/openpose/models\",\n\n  // GPU settings\n  numGpu: 1,             // Number of GPUs to use\n  gpuStartIndex: 0,      // Starting GPU index (usually 0)\n\n  // Resolution average settings\n  numAverages: 1,\n  averageGap: 0.25,\n\n  // Display and network resolution settings\n  renderPose: 0,\n  outputSize: \"-1x-1\",\n  netInputSize: \"-1x256\",\n\n  // Model settings\n  poseMode: 1,\n  poseModel: \"BODY_25\",\n\n  // Advanced display settings\n  alphaPose: 0.5,\n  alphaHeatmap: 0.5,\n  partToShow: 0,\n  renderThreshold: 0.05,\n  fpsMax: -1,\n  upSamplingRatio: 0.0\n);\n</code></pre> <p>Note</p> <p>The <code>modelFolder</code> parameter must point to your OpenPose models directory. This is usually found in the OpenPose installation directory.</p>"},{"location":"getting-started/#step-4-start-tracking","title":"Step 4: Start Tracking","text":"<p>Once configured, you can start tracking:</p> <pre><code>Kinect.startTracking;\n</code></pre> <p>If you want to see what the Kinect is seeing:</p> <pre><code>Kinect.showDisplay;  // Show the camera view with pose overlay\n// Later, to hide it:\nKinect.hideDisplay;\n</code></pre>"},{"location":"getting-started/#step-5-use-joint-positions-in-your-sound","title":"Step 5: Use Joint Positions in Your Sound","text":"<p>Now you can use the <code>Kinect.kr</code> UGen to map joint positions to sound parameters:</p> <pre><code>// Create a SynthDef that uses Kinect data\nSynthDef(\\kinect_controlled_synth, {\n  // Map right wrist Y position (up/down) to frequency\n  var freq = Kinect.kr(200, 800, \"RWrist\", \"Y\");\n\n  // Map left wrist X position (left/right) to amplitude\n  var amp = Kinect.kr(0, 1, \"LWrist\", \"X\");\n\n  // Create a sound using these parameters\n  var sound = SinOsc.ar(freq) * amp;\n\n  // Output the sound\n  Out.ar(0, sound ! 2);\n}).add;\n\n// Create an instance of the synth\nx = Synth(\\kinect_controlled_synth);\n\n// Later, to stop it:\nx.free;\n</code></pre>"},{"location":"getting-started/#step-6-cleanup","title":"Step 6: Cleanup","text":"<p>When you're done using the Kinect:</p> <pre><code>// Stop tracking\nKinect.stopTracking;\n\n// Stop the device\nKinect.stop;\n\n// Close the device\nKinect.closeDevice(\"YOUR_DEVICE_SERIAL\");\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you understand the basics, you can also explore the API Reference for more detailed information about all available functions. </p>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#step-1-install-supercollider","title":"Step 1: Install SuperCollider","text":"<ol> <li>Download and install from the SuperCollider website</li> <li>For building SCKinect, you also need the SuperCollider source code:</li> </ol> <pre><code>git clone https://github.com/supercollider/supercollider.git\n</code></pre>"},{"location":"installation/#step-2-install-cuda","title":"Step 2: Install CUDA","text":"<p>SCKinect requires CUDA for effective performance:</p> <ol> <li>Download and install the CUDA Toolkit from NVIDIA's website. You actually don't want to download the latest and should aim for 11.6 or 11.7 (for compatibility reasons).</li> <li>Make sure to install the drivers. Try to install the latest one if you can. Although the CUDA toolkit will be an older version than the drivers, you can skip the driver install when installing the toolkit since you've already installed the newer drivers. Do this combination for the best compatibility. Higher CUDA versions will consume too much memory or have compilation issues with OpenPose on Linux, and older GPU drivers won't work on a newer GPU :/.</li> <li>Next, you should install cuDNN which is a neural net extension to CUDA. The install process is pretty similar to that of the CUDA install. Follow the instructions on the Nvidia website.</li> <li>Clone the CUDA samples and make note of where the samples folder is located. Make sure to also checkout the version tag which matches the version of your CUDA install. If they don't match, try to pick the closest version below (i.e. 11.6 if you have CUDA 11.7) <pre><code>git clone https://github.com/NVIDIA/cuda-samples.git\ncd cuda-samples\ngit checkout tags/v11.6\n</code></pre></li> <li>Copy this folder to your CUDA toolkit folder <pre><code>sudo cp -r cuda-samples /usr/local/cuda-11.7\n</code></pre></li> <li>Verify the installation by running: <pre><code>nvcc --version\n</code></pre></li> </ol>"},{"location":"installation/#step-3-install-libfreenect2","title":"Step 3: Install libfreenect2","text":"<p>libfreenect2 is the library that enables communication with the Kinect device.</p> <pre><code>git clone https://github.com/OpenKinect/libfreenect2.git\ncd libfreenect2\n</code></pre> <p>Before you install this, you'll want to edit the <code>CMakeLists.txt</code> file in the root directory, <code>libfreenect2</code> to include the CUDA samples folder downloaded in the previous step.</p> <p>Change the line <code>\"${CUDA_TOOLKIT_ROOT_DIR}/samples/common/inc\"</code> to <code>\"${CUDA_TOOLKIT_ROOT_DIR}/cuda-samples/Common\"</code>. This will allow CMake to find the include files for these which are very important. Ever since CUDA 11.6, the samples are not shipped with the toolkit which is why we need to install them separately. Otherwise you will get some error about missing a math_helper.h file and a bunch of other files. If you have a version of CUDA which ships with the samples, then you don't need to worry about this step.</p> <p>Now once that's done, begin installing <code>libfreenect2</code>.</p> <pre><code>sudo apt-get install libusb-1.0-0-dev libturbojpeg0-dev libglfw3-dev\nmkdir build &amp;&amp; cd build\ncmake .. -DCMAKE_INSTALL_PREFIX=/usr\nmake -j`nproc`\nsudo make install\nsudo ldconfig\n</code></pre> <p>For detailed installation instructions and troubleshooting, see the libfreenect2 installation guide.</p>"},{"location":"installation/#step-4-install-openpose-with-cuda-support","title":"Step 4: Install OpenPose with CUDA Support","text":"<p>OpenPose is required for body tracking. Ensure you build it with CUDA support. Nothing else works at the moment, but future plans are in progress to support even more pose tracking methods besides OpenPose. For now, OpenPose works best with CUDA though.</p> <p>Important</p> <p>Remember the location where you installed OpenPose, as you'll need to reference the models directory when configuring SCKinect.</p> <p>Because the link to the OpenPose models isn't working in the original repo, a fork is linked in the clone command below with working downloads for these models. Thanks Alec Dusheck! <pre><code>git clone https://github.com/AlecDusheck/openpose.git\ncd openpose\ngit submodule update --init --recursive\nsudo apt-get install libopencv-dev &amp;&amp; sudo bash ./scripts/ubuntu/install_deps.sh\nmkdir build &amp;&amp; cd build\ncmake .. -DCMAKE_INSTALL_PREFIX=/usr -DGPU_MODE=CUDA -DBUILD_PYTHON=OFF\nmake -j`nproc`\nsudo make install\nsudo ldconfig\n</code></pre></p> <p>For detailed installation instructions and troubleshooting, see the OpenPose installation guide.</p>"},{"location":"installation/#step-5-build-and-install-sckinect","title":"Step 5: Build and Install SCKinect","text":"<p>Now you can build the SCKinect plugin:</p> <pre><code>git clone https://github.com/L42i/SCKinect.git\ncd SCKinect\nmkdir build &amp;&amp; cd build\n</code></pre> <p>Configure the build with CMake:</p> <pre><code>cmake .. -DCMAKE_BUILD_TYPE=Release -DSC_PATH=/path/to/supercollider/source -DCMAKE_INSTALL_PREFIX=/path/to/supercollider/extensions\n</code></pre> <p>Build and install:</p> <pre><code>cmake --build . --config Release\ncmake --build . --config Release --target install\n</code></pre>"},{"location":"installation/#step-6-verify-installation","title":"Step 6: Verify Installation","text":"<ol> <li>Connect your Kinect sensor to your computer</li> <li>Start SuperCollider and run:</li> </ol> <pre><code>// Always need to run the server before operating on UGen!\ns.boot;\n\n// Now server should be booted and we can run this command\nKinect.findAvailable;\n</code></pre> <p>This should list your connected Kinect device with its serial number in the post window.</p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Kinect not detected: Make sure your Kinect is properly connected (i.e. loose USB, missing drivers, missing udev rules, etc.)</li> <li>Build errors: Check the repos above for troubleshooting instructions</li> <li>OpenPose errors: Make sure the path to OpenPose models is correctly specified when calling <code>Kinect.configureTracking()</code></li> <li>Performance issues: Ensure you are using the right netResolution and only tracking as many people as you need to</li> </ol>"},{"location":"installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here, check: - The GitHub repository for open issues</p>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>Now that you have everything installed, you can explore the Getting Started page for a little SuperCollider example with SCKinect! </p>"},{"location":"api/cpp-interface/","title":"C++ Interface","text":"<p>The SCKinect plugin is implemented in C++ and provides a bridge between the Kinect sensor, OpenPose, and SuperCollider. This page documents the C++ components and how they interact with each other.</p>"},{"location":"api/cpp-interface/#key-classes-and-structures","title":"Key Classes and Structures","text":""},{"location":"api/cpp-interface/#kinect-ugen-class","title":"Kinect UGen Class","text":"<pre><code>namespace Kinect {\nclass Kinect : public SCUnit {\npublic:\n    Kinect();\nprivate:\n    // Calc function\n    void next(int nSamples);\n    // Member variables\n};\n}\n</code></pre> <p>This is the main UGen class that SuperCollider interfaces with. It:</p> <ul> <li>Handles the audio rate processing in the <code>next</code> method</li> <li>Maps joint positions to output values</li> <li>Provides the interface for SuperCollider's <code>Kinect.kr</code> method</li> </ul>"},{"location":"api/cpp-interface/#kinectdata-structure","title":"KinectData Structure","text":"<pre><code>struct KinectData {\n    op::WrapperT&lt;op::Datum&gt; opWrapperT; // OpenPose wrapper\n    std::shared_ptr&lt;WUserInput&gt; wUserInput;\n    std::shared_ptr&lt;WUserOutput&gt; wUserOutput;\n    libfreenect2::Freenect2 mFreenect2;\n    libfreenect2::PacketPipeline* mPipeline = new libfreenect2::CpuPacketPipeline();\n    std::unordered_map&lt;std::string, libfreenect2::Freenect2Device*&gt; mOpenDevices;\n    std::string mSelectedSerial;\n    PacketPipeline selectedPipeline = CPU;\n};\n</code></pre> <p>This structure maintains the state of the Kinect connection and OpenPose processing. It contains:</p> <ul> <li>The OpenPose wrapper</li> <li>Input and output handlers for the Kinect frames</li> <li>Libfreenect2 instances for Kinect communication</li> <li>Maps of connected devices</li> <li>Pipeline configuration</li> </ul>"},{"location":"api/cpp-interface/#wuserinput-class","title":"WUserInput Class","text":"<pre><code>class WUserInput : public op::WorkerProducer&lt;std::shared_ptr&lt;std::vector&lt;std::shared_ptr&lt;op::Datum&gt;&gt;&gt;&gt; {\npublic:\n    WUserInput();\n    ~WUserInput();\n    void setDevice(libfreenect2::Freenect2Device* device);\n    void initializationOnThread();\n    std::shared_ptr&lt;std::vector&lt;std::shared_ptr&lt;op::Datum&gt;&gt;&gt; workProducer();\nprivate:\n    libfreenect2::Freenect2Device* mDev;\n    libfreenect2::SyncMultiFrameListener mListener;\n    libfreenect2::FrameMap mFrames;\n};\n</code></pre> <p>This class is responsible for acquiring frames from the Kinect and feeding them to OpenPose.</p>"},{"location":"api/cpp-interface/#wuseroutput-class","title":"WUserOutput Class","text":"<pre><code>class WUserOutput : public op::WorkerConsumer&lt;std::shared_ptr&lt;std::vector&lt;std::shared_ptr&lt;op::Datum&gt;&gt;&gt;&gt; {\npublic:\n    WUserOutput();\n    ~WUserOutput();\n    void initializationOnThread();\n    void workConsumer(const std::shared_ptr&lt;std::vector&lt;std::shared_ptr&lt;op::Datum&gt;&gt;&gt;&amp; datumsPtr);\n    op::Array&lt;float&gt; poseKeypoints;\n    bool isDisplayEnabled = true;\n};\n</code></pre> <p>This class processes the output from OpenPose and makes the pose data available to the Kinect UGen.</p>"},{"location":"api/cpp-interface/#enumerations","title":"Enumerations","text":""},{"location":"api/cpp-interface/#packetpipeline-enum","title":"PacketPipeline Enum","text":"<pre><code>enum PacketPipeline {\n    Dump, CPU, OpenGL, CUDA, CUDAKDE\n};\n</code></pre> <p>This enumeration defines the different processing pipelines available for the Kinect sensor:</p> <ul> <li>Dump: Minimal processing (for testing)</li> <li>CPU: CPU-based processing</li> <li>OpenGL: OpenGL-accelerated processing</li> <li>CUDA: CUDA GPU-accelerated processing</li> <li>CUDAKDE: Enhanced CUDA processing with Kernel Density Estimation filtering</li> </ul>"},{"location":"api/cpp-interface/#jointname-enum","title":"JointName Enum","text":"<pre><code>enum JointName {\n    Nose, Neck, RShoulder, RElbow, RWrist,\n    LShoulder, LElbow, LWrist, MidHip, RHip,\n    RKnee, RAnkle, LHip, LKnee, LAnkle, REye,\n    LEye, REar, LEar, LBigToe, LSmallToe, LHeel,\n    RBigToe, RSmallToe, RHeel\n};\n</code></pre> <p>This enumeration defines the indices for different body joints that can be tracked.</p>"},{"location":"api/cpp-interface/#jointcoordinate-enum","title":"JointCoordinate Enum","text":"<pre><code>enum JointCoordinate {\n    X, Y\n};\n</code></pre> <p>This enumeration defines the available coordinates for joint positions.</p>"},{"location":"api/cpp-interface/#command-handlers","title":"Command Handlers","text":"<p>The C++ interface exposes several command handlers that are called when SuperCollider sends commands to the plugin:</p>"},{"location":"api/cpp-interface/#pipeline-commands","title":"Pipeline Commands","text":"<pre><code>void KinectCmd_setPipeline(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\n</code></pre> <p>These functions handle the <code>setPipeline</code> command from SuperCollider, configuring the appropriate libfreenect2 pipeline. The implementation creates the appropriate pipeline object based on the selected type:</p> <pre><code>// Pseudocode from implementation\nswitch(pipeline) {\n    case CUDA:\n        kinectData-&gt;mPipeline = new libfreenect2::CudaPacketPipeline();\n        break;\n    case CUDAKDE:\n        kinectData-&gt;mPipeline = new libfreenect2::CudaKdePacketPipeline();\n        break;\n    // other cases...\n}\n</code></pre>"},{"location":"api/cpp-interface/#device-commands","title":"Device Commands","text":"<pre><code>void KinectCmd_findAvailable(World* world, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\nvoid KinectCmd_openDevice(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\nvoid KinectCmd_closeDevice(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\n</code></pre> <p>These functions handle device discovery and connection, allowing SuperCollider to find and connect to Kinect devices.</p>"},{"location":"api/cpp-interface/#tracking-commands","title":"Tracking Commands","text":"<pre><code>void KinectCmd_start(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\nvoid KinectCmd_stop(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\nvoid KinectCmd_configureTracking(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\nvoid KinectCmd_startTracking(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\nvoid KinectCmd_stopTracking(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\nvoid KinectCmd_showDisplay(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\nvoid KinectCmd_hideDisplay(World* inWorld, void* inUserData, struct sc_msg_iter* args, void* replyAddr)\n</code></pre> <p>These functions handle the various tracking commands, configuring and controlling the OpenPose tracking process.</p>"},{"location":"api/cpp-interface/#ugen-processing","title":"UGen Processing","text":"<pre><code>void Kinect::next(int nSamples) {\n    // Get the input parameters\n    float minval = in0(0);\n    float maxval = in0(1);\n    int joint = in0(2);\n    int coord = in0(3);\n\n    // Access poseKeypoints from WUserOutput\n    // Map joint position to output range\n    // Write output\n}\n</code></pre> <p>The <code>next</code> method is called at the control rate to process and output the joint position data. It:</p> <ol> <li>Reads the input parameters (min/max range, joint and coordinate indices)</li> <li>Accesses the current pose data from the <code>WUserOutput</code></li> <li>Maps the raw coordinate values to the specified output range</li> <li>Writes the result to the output buffer</li> </ol>"},{"location":"api/cpp-interface/#plugin-registration","title":"Plugin Registration","text":"<pre><code>PluginLoad(KinectUGens) {\n    // Register the UGen\n    registerUnit&lt;Kinect::Kinect&gt;(\"Kinect\");\n\n    // Register command handlers\n    DefinePlugInCmd(\"setPipeline\", KinectCmd_setPipeline, KinectCmd_setPipeline2);\n    DefinePlugInCmd(\"findAvailable\", KinectCmd_findAvailable);\n    // ... additional command registrations\n}\n</code></pre> <p>This section registers the Kinect UGen and all its command handlers with SuperCollider, making them available in the language.</p>"},{"location":"api/cpp-interface/#implementation-notes","title":"Implementation Notes","text":""},{"location":"api/cpp-interface/#thread-safety","title":"Thread Safety","text":"<p>The plugin uses a thread-safe design to handle multiple concurrent operations:</p> <ul> <li>The UGen's <code>next</code> method runs in the audio thread</li> <li>The OpenPose processing runs in a separate thread</li> <li>Communication between threads is managed through thread-safe data structures</li> </ul>"},{"location":"api/cpp-interface/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>The <code>CUDAKDE</code> pipeline provides the best depth but requires CUDA-capable GPU</li> <li>Frame rate is affected by the <code>netInputSize</code> parameter in <code>configureTracking</code></li> <li>Smaller image sizes result in faster processing but potentially less accurate tracking</li> <li>The CPU implementation is still in development (i.e. you really need to have a CUDA enabled GPU to use OpenPose at the moment :/). But CPU version besides OpenPose is on the way :) !</li> </ul>"},{"location":"api/cpp-interface/#hardware-requirements","title":"Hardware Requirements","text":"<p>For optimal performance, the following hardware is recommended:</p> <ul> <li>NVIDIA GPU with CUDA capability 3.0 or higher</li> <li>At least 8GB of GPU memory (8GB+ recommended since cuDNN uses a lot of memory)</li> <li>Recent NVIDIA drivers</li> <li>USB 3.0 port for the Kinect sensor</li> </ul>"},{"location":"api/cpp-interface/#memory-management","title":"Memory Management","text":"<ul> <li>The plugin uses smart pointers (<code>std::shared_ptr</code>) for safe resource management</li> <li>Device connections are tracked in a map to prevent duplicate connections</li> <li>Resources are properly cleaned up when devices are disconnected </li> </ul>"},{"location":"api/kinect-class/","title":"Kinect Class API Reference","text":"<p>This page documents all methods and properties of the <code>Kinect</code> class in SuperCollider. This class provides the interface between SuperCollider and the Kinect sensor hardware.</p>"},{"location":"api/kinect-class/#class-variables","title":"Class Variables","text":"<pre><code>classvar &lt;openDevices, &lt;deviceCount, &lt;packetPipelines, &lt;jointNames, &lt;jointCoordinates;\n</code></pre> <ul> <li>openDevices: Dictionary mapping device serial numbers to device IDs</li> <li>deviceCount: Number of currently connected devices</li> <li>packetPipelines: Dictionary mapping pipeline names to internal IDs</li> <li>jointNames: Dictionary mapping joint names to indices </li> <li>jointCoordinates: Dictionary mapping coordinate names to indices</li> </ul>"},{"location":"api/kinect-class/#ugen-methods","title":"UGen Methods","text":""},{"location":"api/kinect-class/#kr","title":"kr","text":"<pre><code>*kr { |minval=0, maxval=1, joint_name=\"Nose\", joint_coordinate=\"X\"| }\n</code></pre> <p>Creates a control rate UGen that outputs the position of a specific joint.</p> <p>Parameters:</p> <ul> <li>minval (Float): Minimum value of the output range (default: 0)</li> <li>maxval (Float): Maximum value of the output range (default: 1)</li> <li>joint_name (String): Name of the joint to track (default: \"Nose\")</li> <li>joint_coordinate (String): \"X\" or \"Y\" coordinate (default: \"X\")</li> </ul> <p>Returns: A control rate UGen that maps the joint position to the specified range.</p> <p>Example:</p> <pre><code>// Map the Y position of the right wrist (0-1 range)\n~rightHandHeight = Kinect.kr(0, 1, \"RWrist\", \"Y\");\n\n// Use it to control the frequency of a sine oscillator (200-800 Hz)\nSinOsc.ar(~rightHandHeight);\n</code></pre>"},{"location":"api/kinect-class/#device-management-methods","title":"Device Management Methods","text":""},{"location":"api/kinect-class/#findavailable","title":"findAvailable","text":"<pre><code>*findAvailable { }\n</code></pre> <p>Scans for connected Kinect devices and prints their serial numbers to the post window.</p> <p>Example:</p> <pre><code>Kinect.findAvailable;\n// Posts something like: Found device with serial: 065915234247\n</code></pre>"},{"location":"api/kinect-class/#setpipeline","title":"setPipeline","text":"<pre><code>*setPipeline { |pipeline = \"CPU\"| }\n</code></pre> <p>Sets the processing pipeline used for Kinect data. Note this is different than what OpenPose uses.</p> <p>Parameters:</p> <ul> <li>pipeline (String): Name of the pipeline to use for libfreenect2. Options are:</li> <li>\"Dump\": Minimal processing (for testing)</li> <li>\"CPU\": CPU-based processing</li> <li>\"OpenGL\": OpenGL-accelerated processing</li> <li>\"CUDA\": CUDA GPU-accelerated processing</li> <li>\"CUDAKDE\": Enhanced CUDA processing with KDE filtering (recommended)</li> </ul> <p>Example:</p> <pre><code>// Use CUDAKDE pipeline (KDE for Kernel Density Estimation - a fancy filter which makes the depth data look nice)\nKinect.setPipeline(\"CUDAKDE\");\n</code></pre>"},{"location":"api/kinect-class/#opendevice","title":"openDevice","text":"<pre><code>*openDevice { |serial| }\n</code></pre> <p>Opens a connection to a specific Kinect device.</p> <p>Parameters:</p> <ul> <li>serial (String): Serial number of the device to open</li> </ul> <p>Example:</p> <pre><code>Kinect.openDevice(\"065915234247\");\n</code></pre>"},{"location":"api/kinect-class/#closedevice","title":"closeDevice","text":"<pre><code>*closeDevice { |serial| }\n</code></pre> <p>Closes the connection to a specific Kinect device.</p> <p>Parameters:</p> <ul> <li>serial (String): Serial number of the device to close</li> </ul> <p>Example:</p> <pre><code>Kinect.closeDevice(\"065915234247\");\n</code></pre>"},{"location":"api/kinect-class/#start","title":"start","text":"<pre><code>*start { }\n</code></pre> <p>Starts data acquisition from the Kinect device.</p> <p>Example:</p> <pre><code>Kinect.start;\n</code></pre>"},{"location":"api/kinect-class/#stop","title":"stop","text":"<pre><code>*stop { }\n</code></pre> <p>Stops data acquisition from the Kinect device.</p> <p>Example:</p> <pre><code>Kinect.stop;\n</code></pre>"},{"location":"api/kinect-class/#tracking-methods","title":"Tracking Methods","text":""},{"location":"api/kinect-class/#configuretracking","title":"configureTracking","text":"<pre><code>*configureTracking {|loggingLevel=3,\nmaxPeople= -1,\nmodelFolder=\"\",\nnumGpu= -1,\ngpuStartIndex=0,\nnumAverages=1,\naverageGap=0.25,\nrenderPose= -1,\noutputSize=\"-1x-1\",\nnetInputSize=\"-1x368\",\nposeMode=1,\nposeModel=\"BODY_25\",\nalphaPose=0.5,\nalphaHeatmap=0.5,\npartToShow=0,\nrenderThreshold=0.05,\nfpsMax= -1,\nupSamplingRatio=0.0| }\n</code></pre> <p>Configures the OpenPose body tracking system.</p> <p>Parameters:</p> <ul> <li>loggingLevel (Integer): Verbosity level (0-3)</li> <li>maxPeople (Integer): Maximum number of people to track (-1 for no limit)</li> <li>modelFolder (String): Path to OpenPose model files</li> <li>numGpu (Integer): Number of GPUs to use (-1 for auto-detect)</li> <li>gpuStartIndex (Integer): Index of the first GPU to use</li> <li>numAverages (Integer): Number of frames to average for smoother tracking</li> <li>averageGap (Float): Gap between the netInputSize and outputSize which helps transition between the two</li> <li>renderPose (Integer): Whether to render pose overlay (-1 for auto render on CPU or GPU, 0 for don't render at all, 1 for CPU, 2 for GPU)</li> <li>outputSize (String): Output frame size in \"WxH\" format (-1 for native)</li> <li>netInputSize (String): Network input size in \"WxH\" format</li> <li>poseMode (Integer): Pose detection mode</li> <li>poseModel (String): Model to use (e.g., \"BODY_25\")</li> <li>alphaPose (Float): Alpha (transparency) of pose overlay</li> <li>alphaHeatmap (Float): Alpha of confidence heatmap overlay</li> <li>partToShow (Integer): Which body part to focus on (0 for all)</li> <li>renderThreshold (Float): Minimum confidence for a joint to be rendered</li> <li>fpsMax (Float): Maximum frames per second (-1 for no limit)</li> <li>upSamplingRatio (Float): Sampling ratio for confidence map</li> </ul> <p>Performance Tip</p> <p>For better performance on lower-end GPUs, try setting <code>netInputSize</code> to a smaller value like \"-1x128\" and limit <code>maxPeople</code> to 1 if you're only tracking yourself.</p> <p>Example:</p> <pre><code>Kinect.configureTracking(\n  3, 1, \"/path/to/openpose/models\",\n  1, 0, 1, 0.25,\n  0, \"-1x-1\", \"-1x256\",\n  1, \"BODY_25\", 0.5,\n  0.5, 0, 0.05, -1, 0.0\n);\n</code></pre>"},{"location":"api/kinect-class/#starttracking","title":"startTracking","text":"<pre><code>*startTracking { }\n</code></pre> <p>Starts the body tracking process.</p> <p>Example:</p> <pre><code>Kinect.startTracking;\n</code></pre>"},{"location":"api/kinect-class/#stoptracking","title":"stopTracking","text":"<pre><code>*stopTracking { }\n</code></pre> <p>Stops the body tracking process.</p> <p>Example:</p> <pre><code>Kinect.stopTracking;\n</code></pre>"},{"location":"api/kinect-class/#showdisplay","title":"showDisplay","text":"<pre><code>*showDisplay { }\n</code></pre> <p>Shows the visual display with camera feed and pose overlay.</p> <p>Example:</p> <pre><code>Kinect.showDisplay;\n</code></pre>"},{"location":"api/kinect-class/#hidedisplay","title":"hideDisplay","text":"<pre><code>*hideDisplay { }\n</code></pre> <p>Hides the visual display.</p> <p>Example:</p> <pre><code>Kinect.hideDisplay;\n</code></pre>"},{"location":"api/kinect-class/#available-joints","title":"Available Joints","text":"<p>The following joint names are available for tracking with the \"BODY_25\" pose model:</p> Joint Name Description Joint Name Description \"Nose\" Nose \"REye\" Right eye \"Neck\" Base of neck \"LEye\" Left eye \"RShoulder\" Right shoulder \"REar\" Right ear \"RElbow\" Right elbow \"LEar\" Left ear \"RWrist\" Right wrist \"LBigToe\" Left big toe \"LShoulder\" Left shoulder \"LSmallToe\" Left small toe \"LElbow\" Left elbow \"LHeel\" Left heel \"LWrist\" Left wrist \"RBigToe\" Right big toe \"MidHip\" Middle of hips \"RSmallToe\" Right small toe \"RHip\" Right hip \"RHeel\" Right heel \"RKnee\" Right knee \"RAnkle\" Right ankle \"LHip\" Left hip \"LKnee\" Left knee \"LAnkle\" Left ankle"},{"location":"api/kinect-class/#next-steps","title":"Next Steps","text":"<p>Wanna go low level? Check out the C++ Interface to better understand how this UGen works thanks to the SuperCollider plugin interface.</p>"}]}